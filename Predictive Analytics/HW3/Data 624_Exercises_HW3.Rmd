---
title: "Data 624_Exercise 5.11_HW3"
author: "Jiaxin Zheng"
date: "2025-02-17"
output:
  html_document: default
  pdf_document: default
---

# 5.11 Exercises:
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(fpp3)
library(dplyr)
```

## 1. Produce forecasts for the following series using whichever of NAIVE(y), SNAIVE(y) or RW(y ~ drift()) is more appropriate in each case:

- a.Australian Population (global_economy)
- b.Bricks (aus_production)
- c.NSW Lambs (aus_livestock)
- d.Household wealth (hh_budget).
- e.Australian takeaway food turnover (aus_retail).

### a. Australian Population (global_economy)
```{r}
head(global_economy)
```

For the Australian Population data, the RW(~drift()) model is more appropriate for this case. In this case we forecast the population in Australia for next 15 years. It captures the average increase over the years.
```{r}
aus_pop_fc <- global_economy %>% 
  filter(Country == 'Australia') %>% 
  model(RW(Population~drift())) %>% 
  forecast(h = 15) 

aus_pop_fc %>% 
  autoplot(global_economy) +
  labs(title = "Australian Population 15 Year Forecast", x = "Year", y = "Population (millions)")
```

### b.Bricks (aus_production)

```{r}
head(aus_production)
```

```{r}
?aus_production
```

For the aus_production data, the mean(), NAIVE(), SNAIVE() are all appropriate in this case.
```{r}
train <- aus_production %>% 
  filter_index("1992 Q1" ~ "2006 Q4")

beer_fit <- train %>% 
  model(
    Mean = MEAN(Beer),
    `Na誰ve` = NAIVE(Beer),
    `Seasonal na誰ve` = SNAIVE(Beer)
  )

beer_fc <- beer_fit %>%  forecast(h = 14)

beer_fc %>% 
  autoplot(train, level = NULL) +
  autolayer(
    filter_index(aus_production, "2007 Q1" ~ .),
    colour = "black"
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```
### c.NSW Lambs (aus_livestock)

```{r}
?aus_livestock
head(aus_livestock)
```

I considered the NAIVE(), But in this case, SNAIVE() is more appropriate.
```{r}
NSW_fc <- aus_livestock %>% 
  filter(Animal == "Lambs", State == "New South Wales") %>% 
  model(SNAIVE(Count)) %>% 
  forecast(h = "5 years")

NSW_fc %>% 
  autoplot(aus_livestock) +
  labs(title = "Australian Lamb Production 5 Year Forecast")
```

### d.Household wealth (hh_budget).

```{r}
?hh_budget
head(hh_budget)
```

I considered about SNAIVE(), but I think NAIVE() is more appropriate, because the data is in time series.
```{r}
household_wealth_fc <- hh_budget %>% 
  model(NAIVE(Wealth)) %>% 
  forecast(h = "5 years") 

household_wealth_fc %>% 
  autoplot(hh_budget) +
  labs(title = "Household Wealth Forecast Next 5 Years")
```


### e.Australian takeaway food turnover (aus_retail).

```{r}
?aus_retail
head(aus_retail)
```

This case, I use RW(y ~ drift()).
```{r}
aus_takeaway <- aus_retail |>
  filter(Industry == "Takeaway food services") %>% 
  select(State, Month, Turnover)

aus_takeaway_fc <- aus_takeaway %>% 
  model(RW(Turnover~drift())) %>% 
  forecast(h = 20) 

autoplot(aus_takeaway, Turnover) +
  autolayer(aus_takeaway_fc)
```

## 2.Use the Facebook stock price (data set gafa_stock) to do the following:

- a.Produce a time plot of the series.
- b.Produce forecasts using the drift method and plot them.
- c.Show that the forecasts are identical to extending the line drawn between the first and last observations.
- d.Try using some of the other benchmark functions to forecast the same data set. Which do you think is best? Why?



a.Produce a time plot of the series.
```{r}
?gafa_stock
head(gafa_stock)
```

```{r}
fb_stock <- gafa_stock %>%
  filter(Symbol == "FB") 

autoplot(fb_stock, Close) +
  labs(y = "$US",
       title = "Facebook Stock Prices")
```

b.Produce forecasts using the drift method and plot them.
```{r}
fb_monthly <- gafa_stock %>%
  filter(Symbol == "FB", !is.na(Close)) %>%
  index_by(Month = yearmonth(Date)) %>%
  summarise(Close = mean(Close))

# Fit drift model
fb_fc <- fb_monthly %>%
  model(Drift = RW(log(Close) ~ drift())) %>%
  forecast(h = 6) %>%
  mutate(.median = median(fb_monthly$Close, na.rm = TRUE))

# Plot
fb_fc %>%
  autoplot(fb_monthly, level = 80) +
  geom_line(aes(y = .median), data = fb_fc, linetype = 2, color = "blue") +
  labs(
    title = "Facebook Monthly Closing Stock Price Forecast)",
    y = "Close Price ($US)"
  )
```


c.Show that the forecasts are identical to extending the line drawn between the first and last observations.

```{r}
ggplot(fb_stock, aes(x = Date)) +
  geom_line(aes(y = Close)) +
  geom_segment(
    aes(x = min(Date), y = first(Close),
        xend = max(Date), yend = last(Close)),
    color = "red", linetype = "dashed"
  ) +
  labs(
    title = "Facebook Stock Closing Prices",
    y = "Close Price ($US)",
    x = "Date"
  )
```

d.Try using some of the other benchmark functions to forecast the same data set. Which do you think is best? Why?

- Because Facebook stock price are follow a random patterns. I think NAIVE() model is the best. It has lowest RMSE and MAE, that means the forecast is closest to the true. 

```{r}

fb_train <- fb_monthly %>%
  filter_index("2014 Jan" ~ "2017 Dec")

fb_fit <- fb_train %>%
  model(
    Mean = MEAN(Close),
    Naive = NAIVE(Close),
    Seasonal_Naive = SNAIVE(Close),
    Drift = RW(Close ~ drift())
  )

# Forecast 12 months ahead
fb_fc <- fb_fit %>% forecast(h = 12)

fb_fc %>%
  autoplot(fb_monthly, level = NULL) +
  labs(
    title = "Forecasts for Monthly Facebook Stock Price",
    y = "Close Price ($US)"
  ) +
  guides(colour = guide_legend(title = "Model"))

```
```{r}
accuracy(fb_fit)
```


## 3. Apply a seasonal na誰ve method to the quarterly Australian beer production data from 1992. Check if the residuals look like white noise, and plot the forecasts. The following code will help. What do you conclude?
- In the time plot, there is no clear pattern, or trends.
- In ACF plot most autocorrelation bars are within the dashed blue line.
- The residuals is a normal distributed.

```{r}
# Extract data of interest
recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)
# Define and estimate a model
fit <- recent_production |> model(SNAIVE(Beer))
# Look at the residuals
fit |> gg_tsresiduals()
# Look a some forecasts
fit |> forecast() |> autoplot(recent_production)
```

## 4.Repeat the previous exercise using the Australian Exports series from global_economy and the Bricks series from aus_production. Use whichever of NAIVE() or SNAIVE() is more appropriate in each case.

- I don't think NAIVE() or SNAIVE() will work in this case, the residuals is no patterns, no trends, or any seasonal patterns.

## 7.For your retail time series (from Exercise 7 in Section 2.10):

a. Create a training dataset consisting of observations before 2011 using

```{r}
set.seed(12345678)
myseries <- aus_retail |>
  filter(`Series ID` == sample(aus_retail$`Series ID`,1))

myseries_train <- myseries |>
  filter(year(Month) < 2011)
```


b. Check that your data have been split appropriately by producing the following plot.

```{r}
autoplot(myseries, Turnover) +
  autolayer(myseries_train, Turnover, colour = "red")
```
c. Fit a seasonal na誰ve model using SNAIVE() applied to your training data (myseries_train).

```{r}
head(myseries_train)
```


```{r}
fit <- myseries_train |>
  model(SNAIVE(Turnover))
```

d. Check the residuals. Do the residuals appear to be uncorrelated and normally distributed?
- I don't think residuals appear to be uncorrelated. Time plot shows some parterres. The histogram of residuals shows approximately normal distribution. 

```{r}
fit |> gg_tsresiduals()
```

e. Produce forecasts for the test data

```{r}
fc <- fit |>
  forecast(new_data = anti_join(myseries, myseries_train))
fc |> autoplot(myseries)
```

f. Compare the accuracy of your forecasts against the actual values.

```{r}
fit |> accuracy()
```
```{r}
fc |> accuracy(myseries)
```


g.How sensitive are the accuracy measures to the amount of training data used?
- The accuracy measures are highly sensitive to the amount of training data used and to the sample size. If the sample is too small or has less training data, the model fails to generalize well to new data. If the sample is too large, it is possible overfitting to the training data.









